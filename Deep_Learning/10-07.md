# Error Backpropagation
- 오차 역전파
## 복습

![image](https://user-images.githubusercontent.com/110795502/194441438-8736125c-2754-42d7-b5ae-f53da2bf50df.png)


- Learning의 목적: Parameter Update (Error down)
  - 자동화 원리: Gradient Descent
  - W - W - r (d Cost/d W) --> 편미분 값 필요 

## 역전파
### 목적
- 수치미분을 수행하지 않고 학습에 필요한 편미분 값을 획득하게 한다

### 원리 
1. 미분 연쇄법칙: 합성함수의 미분 --> 개별함수들의 미분의 곱
2. 역전파 algorithm: 순전파에서 계산된 값과 error의 조합으로 미분값 계산

## Forward Propagation
- the simpkest form of ANN

  ![image](https://user-images.githubusercontent.com/110795502/194441744-da1409db-084c-4009-a15a-bd81d8d559dd.png)
  
 - 각 학습 단계의 parameter update를 위해서 parameter 별 편미분값 필요
  - gradient descent: W - W - r (d Cost/d W)
 - 만약 parameter 개수가 100만개 이상이라면 수치미분으로 가능한가?
 - 가능하다 해도 많은 컴퓨팅 자원과 시간이 소요
 - MLP에서는 Hideen Layer와 Node의 개수에 따라 Parameter의 개수 증가
 - Parameter 개수는 Model Capacity에 영향을 줌
 - 하지만 Parameter 개수의 증가는 학습 시간에 부정적 영향 

## Chain Rule
### previous: 모델

![image](https://user-images.githubusercontent.com/110795502/194442386-79d536f7-6144-47d6-be58-a1ffca05d73e.png)

- hidden layer의 함수가 계속 합쳐지는게 합성함수다 

## 개념
- Multi-Layer Perceptron의 구조적인 한계점 극복
- 미분의 연쇄 법칙(chain Rule)을 반복적으로 적용하는 알고리즘
  - Neural Network는 간단한 함수들의 중첩(합성 함수)으로 구성
  - `합성 합수 미분은 합성 함수를 구성하는 **개별 함수 미분의 곱**으로 처리

  ![image](https://user-images.githubusercontent.com/110795502/194442666-15d3be1e-0cc8-4789-8b03-0f9fa6dfa42a.png)
  
  ![image](https://user-images.githubusercontent.com/110795502/194442739-6f32ac47-8c5d-4537-a6cd-090e4174ea26.png)

## Backpropagation Algorithm 
- Forward porpadation을 수행하여 y_hat 계산
  - y와 y_hat을 사용하여 오차값을 계산 
- 학습: 오차값이 감소하는 방향으로 weight 수정 
  - 효율적인 경사값 계산을 위해여 backpropagation 수행 
  - parameter update를 위해 출력츠으이 오차값을 은닉층으로 전달 
- 미분의 연쇄법칙 + 오차 역전파 알고리즘
  - `수치미분 과정 없이 학습에 필요한 경사값(편미분값) 계산`
  - hidden layer 나 node가 증가해도 빠른 속도로 학습(parameter update) 가능

### 증명 
- 오차값(y-y_hat)을 사용하여 W21, w11 parameters update

![image](https://user-images.githubusercontent.com/110795502/194445420-be59aad8-fac0-438a-96d2-8767773ed9eb.png)


### W21 parameter update

![image](https://user-images.githubusercontent.com/110795502/194445603-3d828089-9db6-4eac-9151-1b076e4e74f2.png)

![image](https://user-images.githubusercontent.com/110795502/194445959-1172a854-e570-4de6-bc06-98a1cb608805.png)


- Ws: weight Sum
- SA: sigmoid activation

#### 연쇄 법칙 적용

![image](https://user-images.githubusercontent.com/110795502/194446633-74361335-7ed0-47f1-adb9-8cdfdd0aeeb2.png)

- n3의 예제 --> 모든 노드를 지나는 역전파를 나타내야 하지만 일일이 추시 미분하는 것 보다 빠르다 

- 빨간색의 미분값은 오차값과(error) 같다
  
  ![image](https://user-images.githubusercontent.com/110795502/194446970-135a0584-88dd-424d-bf1b-465109411bb1.png)
  - 예측값 - 실제값 = error
  - error는 back propagation으로 전달 받는다 
- 파란색의 미분값은 출력값(1-출력값)과 같다 
 
 ![image](https://user-images.githubusercontent.com/110795502/194447180-623701a3-2f75-4536-bc72-72892e9e4a5a.png)

- 보라색의 미분값은 출력값(앞의 노드의 출력값)과 같다
 
 ![image](https://user-images.githubusercontent.com/110795502/194447118-a2cd30d7-0536-4868-b77e-ef2c31fed0c9.png)

- forward propagation 과정에서 생성된 결과들의 조합 패턴 
- 출력층 가중치 업데이트 (gradient descent)
   
   ![image](https://user-images.githubusercontent.com/110795502/194447390-27949853-e824-4f7f-9ee9-3a2976aab1b6.png)

![image](https://user-images.githubusercontent.com/110795502/194447972-b591c6f0-822b-4c5f-9542-4603215bbe99.png)

## Vanishing Gradient 
> layer가 깊어지면 0에 수렴하는 편미분 값때문에 학습이 안되는 상황이 발생


- sigmoid의 미분값이 가지는 값은 0~0.25사이의 값이다
- 0~0.25 사이의 값을 계속 곱하면 0으로 수렴한다 --> vanishing gradient 

![image](https://user-images.githubusercontent.com/110795502/194451995-f19a0df3-bc3d-496e-97a9-2038d6c2ba89.png)

- 역전파는 출력충으로부터 하나씩 앞으로 돌아오면서 각 층의 가중치를 학습
  - 신경망 모델의 가중치 학습에 미분값, 즉 기울기가 필요
  - sigmoid 함수를 미분하면 최대치가 0.25로 1보다 작은값 생성
  - 은닉층이 증가하면 `미분값(기울기)가 0이되는 문제가 발생
  - 0 이되면 학습이 안된다
- 대응책: Activation Function(sigmoid)을 다른 함수로 대체하여 학습

### activation functions

![image](https://user-images.githubusercontent.com/110795502/194456723-3b692073-eccc-4f19-b679-58db374c516c.png)

![image](https://user-images.githubusercontent.com/110795502/194452421-a1615b75-0080-4311-8746-ab36c247c050.png)

- Tanh 함수의 편미분 값은 sigmoid보다 더 크기 때문에 학습이 된다 

# Optimization Method
> 학습의 성능을 더 좋게 만들기 위해 경사하강법을 변형해서 처리한다 
- 최적화: 모델이 데이터를 설명할 수 있게 하는 것

## Method

![method](https://user-images.githubusercontent.com/110795502/194455770-fb9d9f65-6bcb-42f2-aa19-ddaa95403741.gif)

1. 확률적 방식
  - Stochastic Gradient Descent(SGD)
2. 관성 방식
  - Momentum
- Nesterov Momentum
3. 적응 방식
  - Adaptive Gradient(Adagrad)
  - RMSProp
4. 1+2+3
  - Adam(Adaptive Moment Estimation)
  - AdaDelta 

### Stochastic Gradient Descent(SGD)
- 전체 데이터(batch) 대신 일부 데이터(mini-batch)를 사용 (확률로 일부 데이터를 뽑음)
- batch GD보다 부정확할 수 있지만 계산 속도가 훨씬 빠름
  - 최솟값이 
- 같은 시간에 더 많은 step을 이동 가능
- 일반적으로 batch의 결과에 수렴
- 현재 다양한 SGD의 변형이 존재
- 경사하강 식이 바뀌지 않음  
  
  ![image](https://user-images.githubusercontent.com/110795502/194456405-b6671451-19c9-4503-b83b-8665ba17fe27.png)


### Momentum
- 이동 과정에 관성을 반영
- 현재 이동하는 방향과 별개로 `과거 이동 방식을 기억`하여 이동에 반영 
- 이전 가중치의 업데이트양(벡터)과 방향이 크게 변화하지 않도록 조정
- 바로 직전 시점의 가중치 업데이트 변화량을 적용

![image](https://user-images.githubusercontent.com/110795502/194456860-415f51dd-3920-4543-9d0f-d3596edaa683.png)

### Adaptive Gradient(Adgrad)
- 학습률이 작으면 안정적이지만 학습 속도는 느려짐
- 학습 횟수가 증가함에 따라 학습률을 조절하는 옵션 추가
  - 처음에는 컸다가 점점 작아지게 만든다 
- 학습률 감쇠식 추가 

  ![image](https://user-images.githubusercontent.com/110795502/194457331-964a76de-86e2-417b-99c0-577fe7aca053.png)


### Root Mean Square Propagation(RMSProp)
- Adagrad의 단점인 Gradient 저곱합을 지수평균으로 대체

### Adaptive Moment Estimation(Adam)
- SGD와 RMSProp과 Momentume 방식의 장점을 합친 알고리즘
- Momentum과 같이 지금까지 계산해온 Gradient의 지수평균을 저장
- RMSProp과 같이 Gradient 제곱값의 지수평균을 저장 


















# Deep Learning 
## Agenda
1. Artificial Neural Network (ANN)
2. Error Backpropagation
3. Optimization Method
4. TensorFlow
5. Deep Neural Network (DNN)
6. Convolutional Neural Network (CNN)
7. Recurrent Neural Network (RNN)
8. Long Short-Term Memory (LSTM)
9. Generative Adversarial Network (GAN)

## Brain Storming
- Unstructured Data Modeling (비정형 데이터 모델링)
- 머신러닝 알고리즘에 딥러닝이 포함되어 있다
    - 머신 러닝의 지도학습 '신경망 알고리즘'의 확장이 딥러닝이다 

# Artificial Neural Network
## SUM
- 한쪽 방향으로 흐름
- 1개 이상의 hidden layer을 가짐
    - input, hidden, output layer 등의 여러층을 가짐 -> Multi layer
- 각 층 사이에는 다 연결되어 있음 (fully connected)
    - 그러나, 같은 층 사이에는 연결이 없음 (bi-partie graph)
- 위의 특징들때문에 `Multi Layer Perception` 이라 부른다
- hidden layer
    - 데이터의 패턴을 찾는 역활을 한다
    - 추가적인 작업을 통해 종속변수의 정확한 예측에 기여하는 node들을 뽑아낸다
    - 앞 layer에 존재하는 정보들 중 종속변수 예측에 중요한 정보들을 추출해주는 역할을 한다.
    - HY의 개수가 많을 수록 deep한 neural network가 생성되어 서능이 높아지는 반면
    - HY가 너무 많으면 의사결정을 하지 못하게 됨 (시간도 오래 걸리고)
    - 보통 hidden layer 수가 많은 경우를 ‘deep neural networks’ = ‘deep learning’이라고 부른다
- hidden layer 작동 방식
    - 이전 layer의 output과 hidden layer의 weight maxtrix를 곱하고, 비선형함수에 (sigmoid) 넣는다

## Neural Network
> 함수를 여러개 연결해서 학습시키는 것 

- 인공 신경망 (Artificial Neural Network)
    - 머신러닝 분야에서 연구되는 학습 알고리즘
    - 수치예측, 범주예측, 패턴 인식, 제어 분야에 응용
- 인간의 뇌 구조를 모방하여 만들어짐 (신경세표: Neuron)
    - 수상돌기 (Dendrite), 시냅스 (Synapse), 신경세포체 (Soma), 축색 (Axon)
        - **수상돌기 = 입력 (x), 축색 = 출력 (y)**
        - **시냅스 = 가중값을 갖는 연결 네트워크 (w), 신경세포체 = 노드 (함수)**

### 생물학적 신경망 vs. 인공 신경망 

![](2022-10-06-09-33-04.png)

- 다중회귀로 자극 (x값을) 다 더해서 y값으로 출력을 준다  
--> logistic regression이랑 별반 다르지 않다 

![](2022-10-06-09-34-44.png)

- activation function = sigmoid 

## Deep Learning 
- 심층 학습: 여러개의 함수들이 hidden-layer 두개 이상으로 연결되어 
- parameter의 개수를 증가시키기 위해서 여러개의 함수를 연결해서 modeling 

## Perceptron = Node = sigmoid 함수
- 정의
    - 인공신경망의 한 종류 (선형 분리기)
    - 가장 간단한 형태의 forward network (왼쪽에서 오른쪽으로 방향성)
    - 1957년 프랑크 로젠블라트에 의해 고안
- 동작원리
    - 노드의 입력값(input)과 가중치(weight)의 곱을 모두 합함
    - 합한 값이 활성화 함수의 임계치보다 크면 1 작으면 0을 출력
    - y = sigmoid(w1x1+w2x2+w3x3+...+b) --> 0~1

### Logic Gate (진리표)
- True: 1 / False: 0

    ![](2022-10-06-09-43-59.png)

- NAND: AND의 반대
- XOR: 두개가 같으면 거짓, 다르면 참

- perceptron으로 학습을 해서 logic gate를 만들어내는데 성공함 

#### perceptron: 

![](2022-10-06-09-49-24.png)

- 학습 parameter의 update

#### Classification - AND
- Learning Algorithm: AND로 동작하기 위한 W와 B의 값을 찾는것 

    ![](2022-10-06-09-56-05.png)

    - W와 B는 경사하강법으로 바뀌면서 학습한다 
    - 초록색 선이 perceptron 함수 
    - true인 지역과 false인 지역을 나누는 것이 선의 목적 

    ![](2022-10-06-09-58-47.png)

#### Perception - AND, OR
- ŷ = sigmoid(WX+b)
    + 수학에서 대문자로 표시된 영문자는 행렬이다 

    ![](2022-10-06-10-00-36.png)

#### Perception - XOR
- XOR은 perceptron으로 학습이 안된다
- 선 하나로 true인 지역과 false인 지역으로 나눌 수 없다 

    ![](2022-10-06-10-20-13.png)

### learning(): 경사하강

![](2022-10-06-11-22-10.png)

` w = w - r (d*Error / d*w)`

- (d*Error / d*w): gradient 
- Error = MSE : mean(y-ŷ)^2

#### 미분
- 일반 미분
    - 접선의 기울기 = 순간 변화량 (밑변/높이)

        ![](2022-10-06-11-12-36.png)


- 수치 미분 

    ![](2022-10-06-11-17-07.png)

    - error 함수가 계속 바뀌기에 수치 미분으로 하는 것

- 편미분 (변화량)
    - 변수가 2개 이상인 가변수 함수 
        - 대상 변수 1개을 제외한 나머지는 상수로 취급해 진행하는 방정식 

            ![](2022-10-06-11-25-31.png)

    - 데이터 분석의 관점에서 변수가 바뀌면 결과가 어떤 변화량을 가지냐를 나타낸다
        + ex) 체중 = 운동량 + 식사량 -> 운동량이 변화에 따른 체중의 변화량을 보고 싶은 것
        - d Error / d w: w의 변화에 따른 error의 변화량을 알고 싶은 것이다 
        - d Error / d b: b의 변화에 따른 error의 변화량을 구하는 것
        - 변화량: Gradient 

#### Perceptron - XOR

- NAND , OR , AND를 연결시켜 사용하여 문제 해결 

![image](https://user-images.githubusercontent.com/110795502/194217134-40f3d1bb-7ab1-48df-991e-1214517150c4.png)

![image](https://user-images.githubusercontent.com/110795502/194217173-8adae7d8-0a32-4eb7-b03d-b58a55381216.png)


##    Multi-Layer Perceptron: 
- hinden layer 가 1개 
- linear model: regression
    ![image](https://user-images.githubusercontent.com/110795502/194217015-b386864a-8b5e-4db0-9a12-c507109b6c9f.png)


- Nonlinear Model

![image](https://user-images.githubusercontent.com/110795502/194216021-4bdd9653-1350-45cc-b3a3-9b4e712e6d6e.png)
    
    -  하나를 쓰는 것이 아니라 함수를 많이 써 더 복잡한 문제를 해결하려 하는 것 

    - 선 하나하나가 w이다
    - parameter: 3 x 4 + 4 +1 = 21개 
    - perceptron: sigmoid 함수의 갯수 = 4 개
    - 한번 학습할때 마다 경사하강이 (미분) 21번 되야 한다 --> parameter가 늘어나 성능은 늘지만 시간도 늘어난다 

- layer을 많이 하려는 이유는 parameter를 많이 사용하고 싶어서 그런 것
- 다중 퍼셉트론: 퍼셉트론으로 해결할 수 없는 비선형 분리 문제에 필요
- 여러 층(layer)의 퍼셉트론(node)을 쌓아서 동작 (Input - Hidden - Output)

    
    ![image](https://user-images.githubusercontent.com/110795502/194216699-df2cd764-cd8b-4beb-a27e-22600d0c8be1.png)

### Deep Neural Network (DNN)

![image](https://user-images.githubusercontent.com/110795502/194216791-ba9d8d32-936a-4f44-bc0c-39efa547662a.png)

### MLP 
- Parameter 학습법: Gradient Descent
    - 경사하강법을 사용하여 w와 b를 학습
    
    ![image](https://user-images.githubusercontent.com/110795502/194219523-e34294b9-8afa-4015-8e80-0cdef812a6d4.png)
    
    ![image](https://user-images.githubusercontent.com/110795502/194219616-053a93f9-38bf-40ac-97d6-71b9c85f6781.png)
    
#### Tensor
- Multi Dimensional Matrix


- hinden layer가 없으면 함수를 하나만 쓰는 것이다

    ![image](https://user-images.githubusercontent.com/110795502/194224657-bc238dd4-4255-4621-9593-0ee82bff5902.png)


##### playground.tensor

- 머신 러닝 방식

![image](https://user-images.githubusercontent.com/110795502/194225595-a9a2a05e-dfc8-4956-8d2e-3f1527bdfa5a.png)

![image](https://user-images.githubusercontent.com/110795502/194225511-47ada17c-6adb-4e5c-a35b-4e86dba8421d.png)

![image](https://user-images.githubusercontent.com/110795502/194225650-fe5d936a-2862-4746-929c-40459635b643.png)

- 딥러닝 방식

![image](https://user-images.githubusercontent.com/110795502/194225798-6419d89c-7aca-420b-96e9-6e9b3314863a.png)

- 간단한 함수(node) 3개 (parameter: 6개)를 이용해서 푼다

![image](https://user-images.githubusercontent.com/110795502/194226971-2479c9d7-0755-4005-83d8-8ae99be25e12.png)

![image](https://user-images.githubusercontent.com/110795502/194227280-4ef5eece-8664-4598-ba75-3c18c8ab954c.png)

- 함수를 많이 

### sklearn - MLP

![image](https://user-images.githubusercontent.com/110795502/194238016-332b17df-2c74-49c4-bc23-a9bba0d57ce2.png)

![image](https://user-images.githubusercontent.com/110795502/194242366-1a8e2946-d762-4e92-b60e-895c83727135.png)


- input - hidden - output
    - input layer = x: (1~4개)
    - hidden layer = node: 5개 (임의로 정할 수 있다) (함수)
    - output layer = y: 3개 (바꿀 수 없다) (함수)
    - Fully Connected network: 모든 모드들이 연결된다 (FC)
    - parameter: 4 x 5 + 5 = 5 x 3 + 3 = 43개
    - logistic: sigmoid
    - node가 무조건 많다고 해서 좋은 것이 아니다

- MLPClassifier 로 학습하려면 



